\chapter{Conclusion}%

This thesis has investigated approaches for illuminating the opaque training
process for deep artificial neural networks by tracking certain metrics while
training is running. Detecting bad configurations early as opposed to comparing
final accuracies after the fact could yield immense savings in computation and
cost.  Furthermore, finding metrics which predict undesirable network behaviour
can be instrumental in guiding research of the theoretical mechanisms underlying
the sometimes remarkable performance of neural models, which to this day remain
poorly understood. Candidate metrics for optimising the learning rate and
detecting network convergence have been introduced and evaluated. In doing so,
deep learning folklore and research in the context of learning rates has been
partially debunked and partially validated.

While more work remains to be done to scope out the validity of the findings
presented here (a task which is simplified by the software presented here),
promising results were obtained for detecting layer convergence, whereas the
proposed learning rate metric could not be confirmed to be of immediate use.
Nonetheless, some evidence was presented for its potential pending further
investigation.

A software library has been designed and implemented for the PyTorch deep
learning framework that frees the researcher or practitioner from some of the
repetitive tasks necessary for experimenting with and applying online metrics.
It provides a model-agnostic API based on a Publisher-Subscriber pattern against
which portable and reusable network metrics can be implemented. These metrics
can be distributed as plugins and thus shared with the community. The library
can also be a building block in more comprehensive deep learning applications
and libraries, such as the web-based Peltarion platform, thus saving engineering
effort.
