\chapter{Experiments in live introspection}
\label{ch:experiments}

This chapter serves the purpose of showcasing the library and validating its
usefulness for actual deep learning research. While the ultimate goal is to have
a set of well-researched metrics in place which can be used live during
training, the way to acquire these metrics requires extensive analysis from many
experiments. Therefore, the goal of this work is to obtain such metrics, not
actually use them. This entails that the library is used primarily for easily
gathering all the data from the necessary experiments, and not for supervising
the actual training.

The chapter is divided between a reproduction part (see
\cref{sec:adam}) and an original research part (see
\cref{sec:detecting_learning_rate_proprocess} and
\cref{sec:detecting_layer_saturation}). In the former, experiments are conducted
for two popular variations of gradient descent are reproduced and extended with
the help of the library. The goal is to fact-check claims made by the authors
and validate said claims on more scenarios than are shown in the respective
publications. This will serve to show how \texttt{ikkuna} could have been
employed for this type of work. The second part will be concerned with employing
the library to investigate hypotheses for diagnosing roadblocks in the training
preprocess. Recall from \cref{sec:motivation} the multitude of hyperparameters
for a deep learning model and training regimen. In this work, we will concern
ourselves with the question of figuring out a good learning rate and when to
stop training layers to reduce computation time.

This thesis is not concerned with advancing the state of the art in
classification. Instead, toy problems are developed in order to prove or
disprove that the proposed metrics have the potential to be useful in guiding
the training. A more thorough evaluation of the results on realistic
architectures and problem sizes would require significantly more time and
computational resources and is left for future work.

\section{Review: Stochastic Gradient Descent}%
\label{sec:review_stochastic_gradient_descent}

In this section, we briefly survey the stochastic gradient descent algorithm.
This algorithm is used (in some variant) for virtually all practical deep
learning models. The name derives from the fact that the model parameters are
updated in the direction of the negative gradient, which is the high-dimensional
derivative of the scalar loss function with respect to the model parameters. It
is stochastic as it only uses a subset of the training data at every time step
and thus only approximates the true gradient which would have to be computed
over the entire dataset to be learned.

In standard stochastic gradient descent, a loss $J$ of some the
model parameters $\boldsymbol\theta$ (here, the layer weights) is computed over the training set of $m$
examples by forming the expectation over the sample loss $L$:

\begin{align}
    J(\boldsymbol\theta) &= \mathbb{E}_{\mathbf{x},y\sim\hat{p}_{\text{data}}} L(\mathbf{x}, y, \boldsymbol\theta) \\
                         &= \frac{1}{m}\sum_{i=1}^{m}L(x^{(i)}, y^{(i)}, \boldsymbol\theta)
\end{align}
The cumulative loss can then be derived for $\boldsymbol\theta$

\begin{align}
    \nabla_{\boldsymbol\theta}J &= \frac{1}{m}\sum_{i=1}^{m}\nabla_{\boldsymbol\theta}L(x^{(i)}, y^{(i)}, \boldsymbol\theta)
\end{align}
per the sum rule of differentiation. The simplest form of parameter update rule
is then

\begin{align}
    \boldsymbol\theta &\leftarrow \boldsymbol\theta - \eta \nabla_{\boldsymbol\theta}J
\end{align}
with the learning rate $\eta$. There is no hard and fast rule on what this
parameter should be, and it is subject of large swathes of literature. Popular
modifications to the vanilla update rule are the use of momentum
\citep{jacobs1988increased}, per-layer learning rates (ibd.), reducing the rate
throughout training, or adapting the
learning rate based on mean and variance of gradients across past time steps
\citep{kingma2014adam}. Nevertheless, most of the time, training begins with an
arbitrarily chosen small learning rate around $0.001$ which is then adapted
either by the aforementioned mechanisms or by search over the parameter space on
a subset of the training data when computationally feasible.

\section{Experimental Methodology}%
\label{sec:experimental_methodology}

For all experiments, \texttt{ikkuna} is used for recording various metrics
during training without having to adapt to any specific model. Experiments are
run on a Google Cloud virtual instance with between $4$ and $8$ Intel Xeon CPUs
and $1$ to $2$  Nvidia Tesla K80 graphics cards with $11$ GB of video memory. The
information captured by \texttt{ikkuna} during training is logged to a MongoDB
schemaless database with the help of the \texttt{sacred} library. The data can
then be analysed off-line with MongoDB's Python interface. Plots are created
with \texttt{matplotlib}. % and \texttt{seaborn}.

\section{Detecting Learning Rate Problems}%
\label{sec:detecting_learning_rate_problems}

\subsection{State of the Art and Related Work}%

\subsection{Ratio-Adaptive Learning Rate Scheduling}%
\label{sub:ratio_adaptive_learning_rate_scheduling}

The first problem to be investigated is that of choosing an appropriate learning
rated. We begin with a brief review of gradient descent and then evaluate how
the update-to-weight ratio of model weights can be used to identify bad learning
rates or automate learning rate selection.

As a first hypothesis, we investigate a claim made by \citet{karpathycs231n} who
states that the ratio between updates and weights is a quantity which should be
monitored and constrained. He suggests a target of $\frac{1}{t} = 10^{-3}$ as a
reasonable value, but to the author's knowledge there has never been a thorough
investigation of this hypothesis. Because of this lack of exploration and the
celebrity of the proponent, this merits further investigation.

It seems intuitive that this target cannot be static throughout training, since
we usually decay the learning rate towards the end, leading to smaller magnitude
of updates (this is a prerequisite for theoretical convergence guarantees for
SGD, see \citet[p. 20]{saad1998online}). As a matter of fact, this can be easily
verified by running training with an update rule that scales each gradient so
that the update hits the target (the network does learn, but to a significantly
smaller final accuracy; see \cref{fig:fixed-ratio-opt}). It is also not clear
whether the target should apply to all layers equally, since this would
constitute a strong regulariser on the network parameters, limiting
expressiveness of the model. For instance, in networks with the (now outdated)
$\text{tanh}$ activation function, the later layer's gradients are larger as
fewer backpropagation steps have been done to them, and for this activation
function at least, each application of the chain rule entails multiplication
with a factor $< 1$ (as the tanh derivative never exceeds $1$ and decreases to
almost $0$ in both limits), so gradients become exponentially small. In fact,
the vanishing gradient problem was the main driver to introduce non-saturating
activation functions.  Other nonlinearites such as the rectified linear unit do
not exhibit the same behaviour, but that does not mean all layers must change at
the same rate.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{gfx/diagrams/experiments/ratio_loss_correlation/alexnetmini_sgd_fixed_ratio_01.pdf}
    \caption[Fixed-Ratio optimisation with AlexNetMini on CIFR10]{AlexNetMini on CIFAR10 at $0.1$ learning rate. The blue line is
    vanilla SGD while the green one is obtained by scaling the gradients to have
the updates meet a target ratio of $10^{-3}$}
    \label{fig:fixed-ratio-opt}
\end{figure}

As a first approach to an evaluation, we want to try and select the learning
rate in such a way as to hit the target postulated by
\citeauthor{karpathycs231n}, but not precisely for each layer, but on average
over all layers. This would allow for more flexibility in the weight updates
compared to fixing each update to the same value.

The experimental setup uses the simplified AlexNet architecture
(\cref{sec:pack-models}) shown in \cref{fig:alexnetmini}. The dataset used is
the well-known CIFAR-10 dataset consisting of $60.000$ $32\times32$ colour images
from ten object categories \citep{krizhevsky2009learning}. The optimisation
algorithm used is plain stochastic gradient descent on minibatches of size $128$. The
dataset does not constitute a hard problem to solve; state of the art accuracies
lie around $95\%$. For this reason, a decision must be made about how to make
the problem hard enough so that improvements to the training schedule can
actually be made. The learning rate has thus been fixed to a high value of $0.2$
which is not the optimal value (a learning rate of $0.1$ solves the problem to
$45\%$ accuracy).

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{gfx/diagrams/neural_network/alexnetmini.pdf}
    \caption[Simplified AlexNet architecture]{The network used in this
    experiment. Image features are extracted by $4$ and $8$ convolutional
    filters, respectively, with ReLU nonlinearities. Maximum pooling is applied
    with a filter and stride size of $2$ leading to a resolution a fourth of the
    original size. The classifier portion employs dropout layers to reduce
    co-adaptation of units and a final softmax activation to map outputs to
    class probabilities in $(0,1)$.}
    \label{fig:alexnetmini}
\end{figure}

In order to validate that there is room for improvement (i.e.~the task is not
too easy), the training has been
run about twenty times for both a constant learning rate and an exponentially
decaying rate according to
\begin{equation}
    \eta_{e+1}  = 0.98^{e+1} \eta_{e},
\end{equation}
$e$ being the epoch index.
The final accuracies after $100$ epochs of training for constant learning rate,
exponential decay are shown in \cref{fig:validation1}. As can be seen, there is
a significant improvement when decaying the learning rate over keeping it
constant.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{gfx/diagrams/experiments/experiment_validation.pdf}
    \caption[Final accuracies after 100 epochs]{Final accuracies after 100
    epochs with a learning rate of $0.2$ and batch size of $128$. The adaptive
    ratio schedule is discussed in\cref{sec:ratio-schedule}}
    \label{fig:validation1}
\end{figure}

\subsubsection{The Adaptive Update-to-Weight-Ratio Schedule}\label{sec:ratio-schedule}%

As a first showcase of the library and a test of the update-weight ratio
hypothesis, an adaptive learning rate based on the aforementioned ratio is
implemented with the help of the library.  We will start by formally describing
the update rule and then show how it is implemented with \texttt{ikkuna}.

Let $l$ be the number of layers with weight matrices associated with them (for
instance linear or convolutional layers, but not activation functions, dropout,
or the like). Let $\left\{W_{i,k} \mid i = 0 \ldots l - 1\right\}$ be the set of weight
matrices at training step $k$.  Let $\eta$ be the base learning rate and
$\frac{1}{t}$ be a target value to which we want the update-to-weigh ratio to
move. Furthermore, let $\gamma \in (0, 1)$ be a decay factor for exponential
smoothing.  Now, let

\begin{equation}
    R_{i,k} = \frac{||W_{i,k} - W_{i,k-1}||_2}{||W_{i,k}||_2}
\end{equation}
be the ratio between the $L2$-Norms of layer $i$'s weight updates before step $k$ step
and the weights at step $k$ themselves.  We then select the new learning rate
for batch step $k+1$ as

\begin{align}
    \eta_{k+1}   = \eta_{k}
                   \left(
                   t \cdot \frac{1}{l}
                   \sum_{i=0}^{l-1} \gamma R_k + (1 - \gamma) R_{k-1}
                   \right)^{-1}
\end{align}
for $k \ge 2$.  This is the average exponentially smoothed update-weight-ratio,
divided by the target range. This learning rate is used for vanilla gradient
descent without any other modifications beyond capping it to some value in case
of very small ratios. The effect of adapting the learning rate according to this
schedule is that the average ratio between the weight updates and the weights
moves towards the target range.  It should be noted that this update rule biases
the learning rate in favour of the smaller layers since all ratios are weighted
equally, regardless of the number of weights.
\cref{fig:schedule-accuracies} displays a set of accuracy traces for each of the
schedules (constant, exponential decay, ratio-adaptive) with different base
learning rates. The network was trained from scratch $5$ times for each combination.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{gfx/diagrams/experiments/ratio_adaptive_accuracies.pdf}
    \caption[Accuracy traces for different schedules on CIFAR-10]{Accuracy traces
    for different schedules on CIFAR-10. Batch size and initial learning rate are
    shown above each subplot. The exponential decay schedule uses a decay factor of
    $0.98$. Chance level is $0.1$. The experiments are run for $100$ epochs
    each.}
    \label{fig:schedule-accuracies}
\end{figure}

The results are not overwhelming, which is unsurprising for such a
simple schedule. For the smaller learning rates, it is not better or worse than
a constant or exponentially decaying schedule. However, for unnecessarily high
learning rates, the adaptive schedule outperforms the constant one, hinting at a
possible signal for identifying too high learning rates. This holds for the
smaller of the three batch sizes, which makes sense as a high batch size is
generally more amenable to a high learning rate, as the larger sample size
reduces the noise in the gradient and makes for a smoother loss landscape as the
gradients for more samples are averaged. On the other hand, the adaptive schedule
is also better than a constant one for very small learning rates on large batch
sizes. So it not only works in preventing too-high learning rates, but also too
low ones. This signal could hence be useful for
identifying inappropriate learning rates in small or large batch sizes.

As an impression of how the library presented in \cref{ikkuna} simplifies a
general implementation of such a learning rate schedule, code is provided here.

When an \verb+Exporter+ is configured for a given model, a
\verb+RatioSubscriber+ (see \cref{tbl:subscribers}) must be added to the message
bus in order for the update-weight-ratio ($R_{i,k}$ in the above equations) to be
published. One can then subscribe them and process the information with this
subscriber:
\begin{lstlisting}[language=Python]
class RatioLRSubscriber(PlotSubscriber):
    def __init__(self, base_lr, smoothing=0.9, target=1e-3, max_factor=500):
        subscription = Subscription(self, ['weight_updates_weights_ratio', 'batch_started'],
                                    tag=None, subsample=1)
        super().__init__([subscription], get_default_bus(),
                         {'title': 'learning_rate',
                          'ylabel': 'Adjusted learning rate',
                          'ylims': None,
                          'xlabel': 'Train step'})
        # exponential moving avg of R_{i,k}
        self._ratios     = defaultdict(float)
        # maximum multiplier for base learning rate (in pathological cases)
        self._max_factor = max_factor
        # exp smoothing factor
        self._smoothing  = smoothing
        # target ratio
        self._target     = target
        # this factor is always returned to the learning rate scheduler
        self._factor     = 1
        self._base_lr    = base_lr

    def _compute_lr_multiplier(self):
        '''Compute learning rate multiplicative. Will output 1 for the first batch since no layer
        ratios have been recorded yet. Will also output 1 if the average ratio is close to 0.
        Will clip the factor to some max limit'''

        n_layers = len(self._ratios)
        if n_layers == 0:   # before first batch
            return 1
        else:
            mean_ratio = sum(ratio for ratio in self._ratios.values()) / n_layers
            # prevent numerical issues and keep current LR in that case
            if mean_ratio <=  1e-9:
                return 1
            else:
                factor = self._target / mean_ratio
                return min(factor, self._max_factor)

    # invoked by the runtime for each incoming message
    def compute(self, message):
        if message.kind == 'weight_updates_weights_ratio':
            # the `key` property for these messages will be the module/layer
            # here we compute the exponential moving average of ratios
            i               = message.key
            R_ik            = message.data
            R_ik_1          = self._ratios[i]
            gamma           = self._smoothing
            self._ratios[i] = gamma * R_ik + (1 - gamma) * R_ik_1
        elif message-kind == 'batch_started':
            # before a batch starts, update the lr multiplier
            self._factor = self._compute_lr_multiplier()

    def __call__(self, epoch):
        return self._factor
\end{lstlisting}

The subscriber implements the \verb+__call__()+ method so it can be dropped into
PyTorch's learning rate scheduler (\verb+torch.optim.lr_scheduler.LambdaLR+).
This learning rate schedule can thus be used in every model, without modification.


\subsection{Effects Of Update-to-Weight-Ratio On Training Loss}%
\label{sub:effects_update_to_weight_ratio_on_training_loss}

We have seen in the previous section that at least for pathological cases the UW
ratio can be used to correct the learning rate to some extent. In this section,
we want to examine how this ratio does or should change during training. As
discussed in \cref{sub:ratio_adaptive_learning_rate_scheduling}, it is unlikely
that a constantly high rate of change to the weights will be beneficial
throughout the entire training. We would therefore like to find a relation
between the loss decrease, the current UW ratio and the point in time during
training. This could help us improve the learning rate schedule developed above
and refine the use of the UW ratio as a signal for inappropriate learning rates.

For this experiment, we learn CIFAR-10 for $75$ epochs, again with a batch size
of $128$, with vanilla SGD and the Adam optimizer and different learning rates.
We use the AlexNetMini architecture again, as well as a larger, more powerful
VGG network (schema in \cref{fig:vgg16}). We employ \verb+ikkuna+ to record
losses, accuracies and UW ratios for each layer automatically during training.
In order to make larger trends visible, we smooth the loss trace with a gaussian
kernel.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{gfx/diagrams/neural_network/vgg.pdf}
    \caption[VGG network with $8$ convolutionsl layers.]{VGG network with $8$ convolutional layers. In
    deviation from the original architecture, there is no final softmax layer.}
    \label{fig:vgg16}
\end{figure}

In the following figures, we plot the loss traces and their inverse derivative
(the amount of decrease) in the upper right, the average weight-update ratio in
the lower right, and a scatter plot of ratio versus loss decrease on the left.
The scatter plot has the time step colour-coded (blue is early, red is late) and
in addition offsets according to time on the $z$-axis. Due to the number of
points (up to $30.000$ time steps times the number of runs), the plots have been
subsampled where many points overlap. This was necessary to render the data for
this document. Care has been taken to not destroy the topology of the data.  The
density in a given area may thus not be entirely accurate. In the UW ratio
plots, the line displayed is the average over multiple runs.

\subsubsection{VGG with Stochastic Gradient Descent}

The plots in for this network only show the first $10.000$ training steps since
nothing of note happens afterwards. Since most of the ratio values cluster
around the same values towards the end of training, logarithmic subsampling was
applied.  For the VGG network, we observe a smooth decrease in the training loss
alongside a decrease in the average update-to-weight ratio. Training basically
stalls after around $10.000$ steps (about $25$ epochs).  We observe the trend
that the UW ratio is initially fairly high and falls off subsequently, which
correlates with a decrease in loss. However, there is no particular value of the
ratio that exhibits any significant correlations beyond other values. The
ratio in the beginning of training is also proportional to the learning rate, as
is to be expected.

It is curious that in the very beginning of training, the UW ratio has the
highest value, but the decrease in loss for these steps increases throughout the
first few batches (i.e. learning accelerates) before tapering off. This is
barely visible in the line plots, but shows as a prominent feature in the
scatter plot. It should be noted that this phenomenon happens on a very small
timescale -- the number of data points in the arc is orders of magnitude smaller
than in the rest of the plot, therefore it is no more than a curiosity. As a
preliminary conclusion, we can affirm that there is no linear correlation
between the ratio and the decrease in loss.

Furthermore, the smallest of the evaluated learning rates converges fastest.
The loss flatlines at $0$ after little more than $4000$ steps while the larger
learning rates need proportionally more time. The difference is marginal
however. The phenomenon might relate to the motivation for annealing the
learning rate: As we approach a local minimum, we need smaller learning rates to
not jump over it in a different direction in every update, but slowly fall into
the minimum itself. This may be an indication that an appropriate learning rate
for this particular problem is $\le 0.01$. None of the configurations exhibit UW
ratios close to Karpathy's suggestion of $10^{-3}$, but with the smallest
learning rate, we get closest, and converge fastest. Perhaps this could motivate
Karpathy's constant.

\begin{figure}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\linewidth]{gfx/diagrams/experiments/ratio_loss_correlation/vgg_sgd_001_0_10000.pdf}
        \caption{UW ratio experiment for VGG with SGD and learning rate $0.01$}
        \label{fig:ratio_loss_corr_vgg_sgd_001}
    \end{subfigure}

    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\linewidth]{gfx/diagrams/experiments/ratio_loss_correlation/vgg_sgd_005_0_10000.pdf}
        \caption{UW ratio experiment for VGG with SGD and learning rate $0.05$}
        \label{fig:ratio_loss_corr_vgg_sgd_005}
    \end{subfigure}

    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\linewidth]{gfx/diagrams/experiments/ratio_loss_correlation/vgg_sgd_01_0_10000.pdf}
        \caption{UW ratio experiment for VGG with SGD and learning rate $0.1$}
        \label{fig:ratio_loss_corr_vgg_sgd_01}
    \end{subfigure}
\end{figure}

\subsubsection{VGG with Adam}

The same experiment has been run with the Adam optimizer \citep{kingma2014adam}
and has been found to produce quite different results. We mainly observe that
the steplike nature of the loss function (sharp decreases at epoch boundaries)
are much less pronounced with the Adam optimizer. This removes the loops  which we see for
SGD from the scatterplots. In absolute terms, Adam begets a much higher UW ratio
in the beginning of training, which falls off quickly to SGD's values.
Convergence takes minimally longer and doesn't occur at all with a learning rate
of $0.1$ (\cref{fig:ratio_loss_corr_vgg_adam_01}). Counterintuitively, the
adaptive optimiser is unable to adapt to the high learning rate and is
outperformed by vanilla SGD. The Adam optimiser keeps running estimates of the
mean and variance of the gradient of each parameter, with exponential smoothing
applied. The learning rate is adjusted for each parameter by multiplying it with
$\frac{\mu_{\text{grad}}}{\sqrt{\sigma_{\text{grad}}} + \epsilon}$. The argument
by \citet{kingma2014adam} is that in locations of high gradient variance, a
lower learning rate is appropriate since the estimate of the gradient at this
spot is noisy and unreliable, so smaller steps are a safer choice. Furthermore,
they claim that the mean of the gradient vanishes closer to an optimum, which is
also where we need to anneal the learning rate lest we jump over the minimum.
The Adam optimiser thus has learning rate annealing built-in. The step size
parameter $\eta$ is supposed to be of less importance since in Adam's parameter
update rule, it gives an upper bound on the size of the steps taken in parameter
space, but it can always be adjusted downward if the gradient variance is too
high. It can also be adjusted upward up to the upper bound if the gradient mean
is high, giving Adam momentum-like behaviour. The primary proposition of
\citet{kingma2014adam} is that manually tuning the learning rate per layer or
even per parameter is no longer necessary, but we see here that cases can be
found where precisely this behaviour of Adam would be needed -- we have set a
too-high learning rate, but Adam fails to compensate for this negligence. This
casts doubt on the universality of the Adam optimiser.\footnote{The hypothesis
that Adam starts to show its merit in the absence of batch normalisation could
not be tested on the VGG architecture, since training did not progress at all
with any learning rate or optimiser when batch normalisation layers were
removed. For the smaller AlexNetMini architecture with added batch
normalisation, Adam did not outperform SGD in the absence of batch norm. To the
contrary, Adam even lead to vanishing gradients when using batch normalisation.}

\begin{figure}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\linewidth]{gfx/diagrams/experiments/ratio_loss_correlation/vgg_adam_001_0_10000.pdf}
        \caption{UW ratio experiment for VGG with Adam and learning rate $0.01$}
        \label{fig:ratio_loss_corr_vgg_adam_001}
    \end{subfigure}

    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\linewidth]{gfx/diagrams/experiments/ratio_loss_correlation/vgg_adam_005_0_10000.pdf}
        \caption{UW ratio experiment for VGG with Adam and learning rate $0.05$}
        \label{fig:ratio_loss_corr_vgg_adam_005}
    \end{subfigure}

    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\linewidth]{gfx/diagrams/experiments/ratio_loss_correlation/vgg_adam_01_0_10000.pdf}
        \caption{UW ratio experiment for VGG with Adam and learning rate $0.1$}
        \label{fig:ratio_loss_corr_vgg_adam_01}
    \end{subfigure}
\end{figure}

Remarkably, the qualitative behaviour is the opposite of SGD, which becomes visible
when omitting the first few hundred training steps. While for SGD (plots omitted
for brevity), higher learning rates lead to higher initial values in the UW
distribution, the opposite seems to hold for the adaptive optimizer. This can be
seen in
\cref{fig:ratio_loss_corr_vgg_adam_001_500,fig:ratio_loss_corr_vgg_adam_005_500,fig:ratio_loss_corr_vgg_adam_01_500}
where the first $500$ training steps are omitted to avoid squishing the
scatter plot because of the quickly decaying arcs in the beginning of training.
A possible explanation is that Adam uses estimates of the mean and variance of
gradients, and possibly overcorrects the learning rate.

\begin{figure}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\linewidth]{gfx/diagrams/experiments/ratio_loss_correlation/vgg_adam_001_500_10000.pdf}
        \caption{UW ratio experiment for VGG with Adam and learning rate $0.01$,
        beginning at step $500$}
        \label{fig:ratio_loss_corr_vgg_adam_001_500}
    \end{subfigure}

    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\linewidth]{gfx/diagrams/experiments/ratio_loss_correlation/vgg_adam_005_500_10000.pdf}
        \caption{UW ratio experiment for VGG with Adam and learning rate $0.05$,
        beginning at step $500$}
        \label{fig:ratio_loss_corr_vgg_adam_005_500}
    \end{subfigure}

    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\linewidth]{gfx/diagrams/experiments/ratio_loss_correlation/vgg_adam_01_500_10000.pdf}
        \caption{UW ratio experiment for VGG with Adam and learning rate $0.1$,
        beginning at step $500$}
        \label{fig:ratio_loss_corr_vgg_adam_01_500}
    \end{subfigure}
\end{figure}

\subsubsection{Other Architectures}

Several analyses were conducted on the data, but not displayed here due to
inconclusive results.
The experiments were run on the AlexNetMini architecture as well, without
results, as the network does not converge within $75$ epochs.
The same experiment on the ResNet18 architecture exhibited the same qualitative
behaviour as the VGG network, except for a larger variance in loss decreases for
a given ratio value. Slicing the training into pieces and graphing every slice
of $5000$ training steps separately reveals nothing beyond a decrease in
absolute value of ratio and loss, which is expected. Any correlations between
the two quantities do not change after the initial few steps.

\subsubsection{Conclusion}

The results from all these experiments do not demonstrate that a ratio value
of $0.001$ is any more significant than others. While it is intuitive that the loss
can only decrease significantly if the weights change accordingly (unless the
loss surface is extremely rough), we find no particular insights as to what
ratio is appropriate for any given point in training.

\subsubsection{Ratio-Adaptive Learning Rate Scheduling}

We now want to check how the UW ratio relates to the loss when we attempt to
artificially fix the ratio value to a target of $0.001$. Since the updates to the
weights in vanilla SGD (see \cref{sec:review_stochastic_gradient_descent}) are a
function of the learning rate and the gradient magnitude, this could be done in
one of two ways
\begin{enumerate}
    \item Scale the gradients with the constant learning rate in mind
    \item Set the learning rate according to the ratio-adaptive schedule from
        \cref{sub:ratio_adaptive_learning_rate_scheduling}
\end{enumerate}

Using the ratio-adaptive schedule results in the behaviour shown in
\cref{fig:ratio_loss_corr_vgg_sgd_0001_adaptive,fig:ratio_loss_corr_vgg_sgd_001_adaptive,fig:ratio_loss_corr_vgg_sgd_01_adaptive}.
In contrast to previous experiments, we also have a configuration with $0.001$
learning rate, dropping the $0.05$ value. We also plot the learning rate caused
by the scheduling over the UW ratio.

Note that for this set of plots, the axis limits are different. This serves to
illustrate that the qualitative behaviour is identical for all learning rates
(ignoring the fast arc at the beginning for $\text{lr}=0.1$). The learning rate
trajectories are very similar; the learning rate rises as the UW ratio
decreases. The primary insight from these experiments is that the empirical UW ratio is
not linearly related to the learning rate. Recall from
\cref{sec:review_stochastic_gradient_descent} that the weight update is given by
\begin{align}
    \eta \nabla_{\boldsymbol\theta}J,
\end{align}
so it is a linear function of the learning rate and the gradient magnitude. We
would therefore expect that a tenfold reduction in learning rate would result in
an equal reduction in the UW ratio. However, the difference is not quite as
pronounced in reality. Considering
\cref{fig:ratio_loss_corr_vgg_sgd_001_adaptive} and
\cref{fig:ratio_loss_corr_vgg_sgd_01_adaptive} we see the learning rate
differing by a factor of $\sim{}5$, whereas the UW ratios only differ by a factor of
$\sim{}2$. The cleft between reality and expectation is even more pronounced for
learning rates $0.001$ and $0.1$. This hints at a qualitative difference between
the paths through parameter space taken by higher vs. lower learning rates. If
we assume the directions are mostly the same, we would also expect the higher
learning rate to traverse them quicker (entailing a larger weight change and
thus a higher UW ratio, but possibly suffering from the usual problems of
too-high learning rate). If instead the path taken by the higher learning rate
is actually less steep than the one taken by the smaller rate, we could observe
the phenomenon documented here.

\begin{figure}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\linewidth]{gfx/diagrams/experiments/ratio_loss_correlation/vgg_sgd_0001_0_11700_ratio_schedule.pdf}
        \caption{UW ratio experiment for VGG with SGD and adaptive learning rate
            from $0.001$}
        \label{fig:ratio_loss_corr_vgg_sgd_0001_adaptive}
    \end{subfigure}

    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\linewidth]{gfx/diagrams/experiments/ratio_loss_correlation/vgg_sgd_001_0_11700_ratio_schedule.pdf}
        \caption{UW ratio experiment for VGG with SGD and adaptive learning rate
            from $0.01$}
        \label{fig:ratio_loss_corr_vgg_sgd_001_adaptive}
    \end{subfigure}

    %% leave this as only 3 fit on one page
    % \begin{subfigure}{\textwidth}
    %     \centering
    %     \includegraphics[width=\linewidth]{gfx/diagrams/experiments/ratio_loss_correlation/vgg_sgd_005_0_11700_ratio_schedule.pdf}
    %     \caption{UW ratio experiment for VGG with SGD and learning rate adaptive
    %         from $0.05$}
    %     \label{fig:ratio_loss_corr_vgg_sgd_005_adaptive}
    % \end{subfigure}

    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\linewidth]{gfx/diagrams/experiments/ratio_loss_correlation/vgg_sgd_01_0_11700_ratio_schedule.pdf}
        \caption{UW ratio experiment for VGG with SGD and adaptive learning rate
            from $0.1$}
        \label{fig:ratio_loss_corr_vgg_sgd_01_adaptive}
    \end{subfigure}
\end{figure}

\subsubsection{Conclusion}

The two experiments with a dynamic learning rate schedule based on the average
ratio of updates and parameters do not yield any definitive insights into
whether any one value should be considered a target metric. It was obvious from
the beginning that as the loss curve approaches its asymptote, the change to the
weights needs to decrease, and indeed this has been demonstrated clearly. It is
not clear at this point whether the UW ratio provides additional information
over other metrics such as the loss curve itself or the norm of the gradients,
but the experiments in influencing the ratio have proven that there is no
obvious dependency between them -- problems can still be learned with a smooth
loss decrease even when influencing the UW ratio via learning rate scheduling.

However, more work would be necessary to check whether a moving target could
help optimisation. We have seen througout this section that  when learning works
well, the ratio decays similarly to a an exponential, which opens the
possibility for scheduling the learning rate with some decay function and a base
value (such as $10^{-3}$).  While this may not aid convergence in cases that are
well-behaved, it could move models out of areas it would otherwise spend too
much time in, or reduce parameter oscillations in the face of highly noisy loss
surfaces.

With the help of \texttt{ikkuna}, one could easily implement a ratio-based
learning rate schedule with e.g. exponential decay built-in an test its
performance on a wide range of models and datasets.

\section{Validating Optimiser Research}%
\label{sec:adam}

In this section, we want to highlight the \texttt{ikkuna}'s usefulness in
scientific research by partially reproducing \citep{kingma2014adam} and
investigating in how far the claims made therein hold true on a range of
problems.
