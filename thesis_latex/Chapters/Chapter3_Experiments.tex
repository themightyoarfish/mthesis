\chapter{Experiments in live introspection}

This chapter serves the purpose of showcasing the library and validating its
usefulness for actual deep learning research. While the ultimate goal is to have
a set of well-researched metrics in place which can be used live during
training, the way to acquire these metrics requires extensive analysis from many
experiments. Therefore, the goal of this work is to obtain such metrics, not
actually use them. This entails that the library is used primarily for easily
gathering all the data from the necessary experiments, and not for supervising
the actual training.

The chapter is divided between a reproduction part (see
\cref{sec:optimization_validation}) and an original research part (see
\cref{sec:detecting_learning_rate_proprocess} and
\cref{sec:detecting_layer_saturation}). In the former, experiments are conducted
for two popular variations of gradient descent are reproduced and extended with
the help of the library. The goal is to fact-check claims made by the authors
and validate said claims on more scenarios than are shown in the respective
publications. This will serve to show how \texttt{ikkuna} could have been
employed for this type of work. The second part will be concerned with employing
the library to investigate hypotheses for diagnosing roadblocks in the training
proprocess. Recall from \cref{sec:motivation} the multitude of hyperparameters
for a deep learning model and training regimen. In this work, we will concern
ourselves with the question of figuring out a good learning rate and when to
stop training layers to reduce computation time.

This thesis is not concerned with advancing the state of the art in
classification. Instead, toy problems are developed in order to prove or
disprove that the proposed metrics have the potential to be useful in guiding
the training. A more thorough evaluation of the results on realistic
architectures and problem sizes would require significantly more time and
computational resources and is left for future work.

\section{Review: Stochastic Gradient Descent}%
\label{sec:review_stochastic_gradient_descent}

In this section, we briefly survey the stochastic gradient descent algorithm.
This algorithm is used (in some variant) for virtually all practical deep
learning models. The name derives from the fact that the model parameters are
updated in the direction of the negative gradient, which is the high-dimensional
derivative of the scalar loss function with respect to the model parameters. It
is stochastic as it only uses a subset of the training data at every time step
and thus only approximates the true gradient which would have to be computed
over the entire dataset to be learned.

In standard stochastic gradient descent, a loss $J$ of some the
model parameters $\boldsymbol\theta$ (here, the layer weights) is computed over the training set of $m$
examples by forming the expectation over the sample loss $L$:

\begin{align}
    J(\boldsymbol\theta) &= \mathbb{E}_{\mathbf{x},y\sim\hat{p}_{\text{data}}} L(\mathbf{x}, y, \boldsymbol\theta) \\
                         &= \frac{1}{m}\sum_{i=1}^{m}L(x^{(i)}, y^{(i)}, \boldsymbol\theta)
\end{align}
The cumulative loss can then be derived for $\boldsymbol\theta$

\begin{align}
    \nabla_{\boldsymbol\theta}J &= \frac{1}{m}\sum_{i=1}^{m}\nabla_{\boldsymbol\theta}L(x^{(i)}, y^{(i)}, \boldsymbol\theta)
\end{align}
per the sum rule of differentiation. The simplest form of parameter update rule
is then

\begin{align}
    \boldsymbol\theta &\leftarrow \boldsymbol\theta - \eta \nabla_{\boldsymbol\theta}J
\end{align}
with the learning rate $\eta$. There is no hard and fast rule on what this
parameter should be, and it is subject of large swathes of literature. Popular
modifications to the vanilla update rule are the use of momentum
\citep{jacobs1988increased}, per-layer learning rates (ibd.), reducing the rate
throughout training, or adapting the
learning rate based on mean and variance of gradients across past time steps
\citep{kingma2014adam}. Nevertheless, most of the time, training begins with an
arbitrarily chosen small learning rate around $0.001$ which is then adapded
either by the aforementioned mechanisms or by search over the parameter space on
a subset of the training data when computationally feasible.


\section{Experimental Methodology}%
\label{sec:experimental_methodology}

For all experiments, \texttt{ikkuna} is used for recording various metrics
during training without having to adapt to any specific model. Experiments are
run on a Google Cloud virtual instance with betwen $4$ and $8$ Intel Xeon CPUs
and $1$ to $2$  Nvidia Tesla K80 graphics cards with $11$ GB of video memory. The
information captured by \texttt{ikkuna} during training is logged to a MongoDB
schemaless database with the help of the \texttt{sacred} library. The data can
then be analysed off-line with MongoDB's Python interface. Plots are created
with \texttt{matplotlib}. % and \texttt{seaborn}.

\section{Detecting Learning Rate Problems}%
\label{sec:detecting_learning_rate_problems}

\subsection{Ratio-Adaptive Learning Rate Scheduling}%
\label{sub:ratio_adaptive_learning_rate_scheduling}

The first problem to be investigated is that of choosing an appropriate learning
rated. We begin with a brief review of gradient descent and then evaluate how
the update-to-weight ratio of model weights can be used to identify bad learning
rates or automate learning rate selection.

As a first hypothesis, we investigate a claim made by \citet{karpathycs231n} who
states that the ratio between updates and weights is a quantity which should be
monitored and constrained. He suggests a target of $\frac{1}{t} = 10^{-3}$ as a
reasonable value, but to the author's knowledge there has never been a thorough
investigation of this hypothesis. Because of this lack of exploration and the
celebrity of the proponent, this merits further investigation.

It seems intuitive that this target cannot be static throughout training, since
we usually decay the learning rate towards the end, leading to smaller magnitude
of updates (this is a prerequisite for theoretical convergence guarantees for
SGD, see \citet[p. 20]{saad1998online}). It is also not clear whether the target
should apply to all layers equally. For instance, in networks with the (now
outdated) tanh activation function, the later layer's gradients are larger as
fewer backpropagation steps have been done to them, and for this activation
function at least, each application of the chain rule entails mutliplication
with a factor $< 1$ (as the tanh derivative never exceeds $1$ and decreases to
almost $0$ in both limtis), so gradients become exponentially small. In fact,
the vanishing gradient problem was the main driver to introduce non-saturating
activation functions.  Other nonlinearites such as the rectified linear unit do
not exhibit the same behaviour, but that does not mean all layers must change at
the same rate.

As a first approach to an evaluation, we want to try and select the learning
rate in such a way as to hit the target postulated by
\citeauthor{karpathycs231n}.

The experimental setup uses the simplified AlexNet architecture
(\cref{sec:pack-models}) shown in \cref{fig:alexnetmini}. The dataset used is
the well-known CIFAR-10 dataset consisting of $60.000$ $32\times32$ colour images
from ten object categories \citep{krizhevsky2009learning}. The opimisation
alogrithm used is plain stochastic gradient descent on minibatches of size $128$. The
dataset does not constitute a hard problem to solve; state of the art accuracies
lie around $95\%$. For this reason, a descision must be made about how to make
the problem hard enough so that improvements to the training schedule can
actually be made. The learning rate has thus been fixed to a high value of $0.2$
which is not the optimal value (a learning rate of $0.1$ solves the problem to
$45\%$ accuracy).

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{gfx/diagrams/neural_network/alexnetmini.pdf}
    \caption[Simplified AlexNet architecture]{The network used in this
    experiment. Image features are extracted by $4$ and $8$ convolutional
    filters, respectively, with ReLU nonlinearities. Maximum pooling is applied
    with a filter and stride size of $2$ leading to a resolution a fourth of the
    original size. The classifier portion employs dropout layers to reduce
    co-adaptation of units and a final softmax activation to map outputs to
    class probabilities in $(0,1)$.}
    \label{fig:alexnetmini}
\end{figure}

In order to validate that there is room for improvement (i.e.~the task is not
too easy), the training has been
run about twenty times for both a constant learning rate and an exponentially
decaying rate according to
\begin{equation}
    \eta_{e+1}  = 0.98^{e+1} \eta_{e},
\end{equation}
$e$ being the epoch index.
The final accuracies after $100$ epochs of training for constant learning rate,
exponential decay are shown in \cref{fig:validation1}. As can be seen, there is
a significant improvement when decaying the learning rate over keeping it
constant.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{gfx/diagrams/experiments/experiment_validation.pdf}
    \caption[Final accuracies after 100 epochs]{Final accuracies after 100
    epochs with a learning rate of $0.2$ and batch size of $128$. The adaptive
    ratio schedule is discussed in\cref{sec:ratio-schedule}}
    \label{fig:validation1}
\end{figure}

\subsubsection{The Adaptive Update-to-Weight-Ratio Schedule}\label{sec:ratio-schedule}%

As a first showcase of the library and a test of the update-weight ratio
hypothesis, an adaptive learning rate based on the aforementioned ratio is
implemented with the help of the library.  We will start by formally describing
the update rule and then show how it is implemented with \texttt{ikkuna}.

Let $l$ be the number of layers with weight matrices associated with them (for
instance linear or convolutional layers, but not activation functions, dropout,
or the like). Let $\left\{W_{i,k} \mid i = 0 \ldots l - 1\right\}$ be the set of weight
matrices at training step $k$.  Let $\eta$ be the base learning rate and
$\frac{1}{t}$ be a target value to which we want the update-to-weigh ratio to
move. Furthermore, let $\gamma \in (0, 1)$ be a decay factor for exponential
smoothing.  Now, let

\begin{equation}
    R_{i,k} = \frac{||W_{i,k} - W_{i,k-1}||_2}{||W_{i,k}||_2}
\end{equation}
be the ratio between the $L2$-Norms of layer $i$'s weight updates before step $k$ step
and the weights at step $k$ themselves.  We then select the new learning rate
for batch step $k+1$ as

\begin{align}
    \eta_{k+1}   = \eta_{k}
                   \left(
                   t \cdot \frac{1}{l}
                   \sum_{i=0}^{l-1} \gamma R_k + (1 - \gamma) R_{k-1}
                   \right)^{-1}
\end{align}
for $k \ge 2$.  This is the average exponentially smoothed update-weight-ratio,
divided by the target range. This learning rate is used for vanilla gradient
descent without any other modifications beyond capping it to some value in case
of very small ratios. The effect of adapting the learning rate according to this
schedule is that the average ratio between the weight updates and the weights
moves towards the target range.  It should be noted that this update rule biases
the learning rate in favour of the smaller layers since all ratios are weighted
equally, regardless of the number of weights.
\cref{fig:schedule-accuracies} displays a set of accuracy traces for each of the
schedules (constant, exponential decay, ratio-adaptive) with different base
learning rates. The network was trained from scratch $5$ times for each combination.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{gfx/diagrams/experiments/ratio_adaptive_accuracies.pdf}
    \caption[Accuracy traces for different schedules on CIFAR10]{Accuracy traces
    for different schedules on CIFAR10. Batch size and initial learning rate are
    shown above each subplot. The exponential decay schedule uses a decay factor of
    $0.98$. Chance level is $0.1$. The experiments are run for $100$ epochs
    each.}
    \label{fig:schedule-accuracies}
\end{figure}

The results are not overwhelming, which is unsurprising for such a
simple schedule. For the smaller learning rates, it is not better or worse than
a constant or exponentially decaying schedule. However, for unnecessarily high
learning rates, the adaptive schedule outperforms the constant one, hinting at a
possible signal for identifying too high learning rates. This holds for the
smaller of the three batch sizes, which makes sense as a high batch size is
generally more amenable to a high learning rate, as the larger sample size
reduces the noise in the gradient and makes for a smoother loss landscape as the
gradients for more samples are averaged. On the other hand, the adaptive schedule
is also better than a constant one for very small learning rates on large batch
sizes. So it not only works in preventing too-high learning rates, but also too
low ones. This signal could hence be useful for
identifying inappropriate learning rates in small or large batch sizes.

As an impression of how the library presented in \cref{ikkuna} simplifies a
general implementation of such a learning rate schedule, code is provided here.

When an \verb+Exporter+ is configured for a given model, a
\verb+RatioSubscriber+ (see \cref{tbl:subscribers}) must be added to the message
bus in order for the update-weight-ratio ($R_{i,k}$ in the above equations) to be
published. One can then subscribe them and process the information with this
subscriber:
\begin{lstlisting}[language=Python]
class RatioLRSubscriber(PlotSubscriber):
    def __init__(self, base_lr, smoothing=0.9, target=1e-3, max_factor=500):
        subscription = Subscription(self, ['weight_updates_weights_ratio', 'batch_started'],
                                    tag=None, subsample=1)
        super().__init__([subscription], get_default_bus(),
                         {'title': 'learning_rate',
                          'ylabel': 'Adjusted learning rate',
                          'ylims': None,
                          'xlabel': 'Train step'})
        # exponential moving avg of R_{i,k}
        self._ratios     = defaultdict(float)
        # maximum multiplier for base learning rate (in pathological cases)
        self._max_factor = max_factor
        # exp smoothing factor
        self._smoothing  = smoothing
        # target ratio
        self._target     = target
        # this factor is always returned to the learning rate scheduler
        self._factor     = 1
        self._base_lr    = base_lr

    def _compute_lr_multiplier(self):
        '''Compute learning rate multiplicative. Will output 1 for the first batch since no layer
        ratios have been recorded yet. Will also output 1 if the average ratio is close to 0.
        Will clip the factor to some max limit'''

        n_layers = len(self._ratios)
        if n_layers == 0:   # before first batch
            return 1
        else:
            mean_ratio = sum(ratio for ratio in self._ratios.values()) / n_layers
            # prevent numerical issues and keep current LR in that case
            if mean_ratio <=  1e-9:
                return 1
            else:
                factor = self._target / mean_ratio
                return min(factor, self._max_factor)

    # invoked by the runtime for each incoming message
    def compute(self, message):
        if message.kind == 'weight_updates_weights_ratio':
            # the `key` property for these messages will be the module/layer
            # here we compute the exponential moving average of ratios
            i               = message.key
            R_ik            = message.data
            R_ik_1          = self._ratios[i]
            gamma           = self._smoothing
            self._ratios[i] = gamma * R_ik + (1 - gamma) * R_ik_1
        elif message-kind == 'batch_started':
            # before a batch starts, update the lr multiplier
            self._factor = self._compute_lr_multiplier()

    def __call__(self, epoch):
        return self._factor
\end{lstlisting}

The subscriber implements the \verb+__call__()+ method so it can be dropped into
PyTorch's learning rate scheduler (\verb+torch.optim.lr_scheduler.LambdaLR+).
This learning rate schedule can thus be used in every model, without modification.


\subsection{Effects Of Update-to-Weight-Ratio On Training Loss}%
\label{sub:effects_update_to_weight_ratio_on_training_loss}

We have seen in the previous section that at least for pathological cases the UW
ratio can be used to correct the learning rate to some extent. In this section,
we want to examine how this ratio does or should change during training. As
discussed in \cref{sub:ratio_adaptive_learning_rate_scheduling}, it is unlikely
that a constantly high rate of change to the weights will be beneficial
throughout the entire training. We would therefore like to find a relation
between the loss decrease, the current UW ratio and the point in time during
training. This could help us improve the learning rate schedule developed above
and refine the use of the UW ratio as a signal for inappropriate learning rates.

For this experiment, we learn CIFAR-10 for $75$ epochs, again with a batch size
of $128$, with vanilla SGD and the Adam optimizer and different learning rates.
We use the AlexNetMini architecture again, as well as a larger, more powerful
VGG network (schema in \cref{fig:vgg16}). We employ \verb+ikkuna+ to record
losses, accuracies and UW ratios for each layer automatically during training.
In order to make larger trends visible, we smooth the loss trace with a gaussian
kernel.

In the following figures, we plot the loss traces and their inverse derivative
(the amount of decrease) in the upper right, the average weight-update ratio in
the lower right, and a scatterplot of ratio versus loss decrease on the left.
The scatterplot has the time step colour-coded (blue is early, red is late) and
in addition offsets according to time on the $z$-axis. Due to the number of
points (up to $30.000$ time steps times the number of runs), the plots have been
subsampled where many points overlap. This was necessary to render the data for
this document. Care has been taken to not destroy the topology of the data.  The
density in a given area may thus not be entirely accurate. In the UW ratio
plots, the different shades represent different training runs.

\subsubsection*{VGG}

The plots in for this network only show the first $15.000$ training steps since
nothing of note happens afterwards. Since most of the ratio values cluster
around the same values towards the end of training, logarithmic subsampling was
applied.  For the VGG network, we observe a smooth decrease in the training loss
alonside a decrease in the average update-to-weight ratio. Training basically
stalls after around $10.000$ steps (about $25$ epochs).  We observe the trend
that the UW ratio is initially fairly high and falls off subsequently, which
correlates with a decrease in loss. However, there is no particular value of the
ratio that exhibits any significant correlations beyond other values. The
ratio in the beginning of training is also proportional to the learning rate, as
is to be expected.

It is curious that in the very beginning of training, the UW ratio has the
highest value, but the decrease in loss for these steps increases throughout the
first few batches (i.e. learning accelerates) before tapering off. This is
barely visible in the line plots, but shows as a prominent feature in the
scatterplot. It should be noted that this phenomenon happens on a very small
timescale -- the number of data points in the arc is orders of magnitude smaller
than in the rest of the plot. As a preliminary conlcusion, we can affirm that
there is no linear correlation between the ratio and the decrease in loss.

Furthermore, the smallest of the evealuated learning rates converges fastest.
The loss flatlines at $0$ after little more than $4000$ steps while the larger
learning rates need proportionally more time. The difference is marginal
however. The phenomenon might relate to the motivation for annealing the
learning rate: As we approach a local minimum, we need smaller learning rates to
not jump over it in a different direction in every update, but slowly fall into
the minimum itself. This may be an indication that an appropriate learning rate
for this particular problem is $\le 0.01$

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{gfx/diagrams/experiments/ratio_loss_correlation/vgg_sgd_001.pdf}
    \caption{UW ratio experiment for VGG with SGD and learning rate $0.01$}
    \label{fig:ratio_loss_corr_vgg_sgd_001}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{gfx/diagrams/experiments/ratio_loss_correlation/vgg_sgd_005.pdf}
    \caption{UW ratio experiment for VGG with SGD and learning rate $0.05$}
    \label{fig:ratio_loss_corr_vgg_sgd_005}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{gfx/diagrams/experiments/ratio_loss_correlation/vgg_sgd_01.pdf}
    \caption{UW ratio experiment for VGG with SGD and learning rate $0.1$}
    \label{fig:ratio_loss_corr_vgg_sgd_01}
\end{figure}

The same experiment has been run with the Adam optimizer \citep{kingma2014adam}
and has been found to produce quite different results. We mainly observe that
the steplike nature of the loss function (sharp decreases at epoch boundaries)
do not occur with the Adam optimizer. This removes the loops  which we see for
SGD from the scatterplots. In absolute terms, Adam begets a much higher UW ratio
in the beginning of training, which falls off quickly to SGD's values.
Convergence takes much longer, never truly reaching $0$ training loss.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{gfx/diagrams/experiments/ratio_loss_correlation/vgg_adam_001.pdf}
    \caption{UW ratio experiment for VGG with Adam and learning rate $0.01$}
    \label{fig:ratio_loss_corr_vgg_adam_001}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{gfx/diagrams/experiments/ratio_loss_correlation/vgg_adam_005.pdf}
    \caption{UW ratio experiment for VGG with Adam and learning rate $0.05$}
    \label{fig:ratio_loss_corr_vgg_adam_005}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{gfx/diagrams/experiments/ratio_loss_correlation/vgg_adam_01.pdf}
    \caption{UW ratio experiment for VGG with Adam and learning rate $0.1$}
    \label{fig:ratio_loss_corr_vgg_adam_01}
\end{figure}

Remarkably, the qualitative behaviour is the opposite SGD, which becomes visible
when omitting the first few hundred training steps. While for SGD (plots omitted
for brevity), higher learning rates lead to higher initial values in the UW
distribution, the opposite seems to hold for the adaptive optimizer. This can be
seen in
\cref{fig:ratio_loss_corr_vgg_adam_001_500,fig:ratio_loss_corr_vgg_adam_005_500,fig:ratio_loss_corr_vgg_adam_01_500}
where the first $500$ training steps are ommitted to avoid squishing the
scatterplot because of the quickly decaying arcs in the beginning of training.
A possible explanation is that Adam uses estimates of the mean and variance of
gradients, and possibly overcorrects the learning rate. We will investigate Adam
more closely in \cref{sec:adam}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{gfx/diagrams/experiments/ratio_loss_correlation/vgg_adam_001_500_29250.pdf}
    \caption{UW ratio experiment for VGG with Adam and learning rate $0.01$,
    beginning at step $500$}
    \label{fig:ratio_loss_corr_vgg_adam_001_500}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{gfx/diagrams/experiments/ratio_loss_correlation/vgg_adam_005_500_29250.pdf}
    \caption{UW ratio experiment for VGG with Adam and learning rate $0.05$,
    beginning at step $500$}
    \label{fig:ratio_loss_corr_vgg_adam_005_500}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{gfx/diagrams/experiments/ratio_loss_correlation/vgg_adam_01_500_29250.pdf}
    \caption{UW ratio experiment for VGG with Adam and learning rate $0.1$,
    beginning at step $500$}
    \label{fig:ratio_loss_corr_vgg_adam_01_500}
\end{figure}

\subsubsection*{Other Architectures}

Several analyses were conducted on the data, but not displayed here due to
inconclusive results.
The experiments were run on the AlexNetMini architecture as well, without
results, as the network does not converge within $75$ epochs.
The same experiment on the ResNet18 architecture exhibited the same qualitative
behaviour as the VGG network, except for a larger variance in loss decreases for
a given ratio value. Slicing the training into pieces and graphing every slice
of $5000$ training steps separately reveals nothing beyond a decrease in
absolute value of ratio and loss, which is expected. Any correlations between
the two quantities do not change after the initial few steps.

The results from all these experiments do not demonstrate that a ratio value
of $0.001$ is any more significant than others. While it is intuitive that the loss
can only decrease significantly if the weights change accordingly (unless the
loss surface is extremely rough), we find no particular insights as to what
ratio is appropriate for any given point in training.

We now want to check how the UW ratio relates to the loss when we attempt to
artificially fix the ratio value to a target of $0.01$. Since the updates to the
weights in vanilla SGD (see \cref{sec:review_stochastic_gradient_descent}) are a
function of the learning rate and the gradient magnitude, this could be done in
one of two ways
\begin{enumerate}
    \item Scale the gradients with the constant learning rate in mind
    \item Set the learning rate according to the ratio-adaptive schedule from
        \cref{sub:ratio_adaptive_learning_rate_scheduling}
\end{enumerate}
