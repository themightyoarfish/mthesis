\chapter{Future Work}%
\label{cha:future_work}

This chapter will examine some avenues of inquiry which were beyond the scope of
this work and scope out future work needed to solidify the findings from
\cref{ch:experiments}.

\section{Extensions To The Experiments}

The \texttt{ikkuna} library was employed in \cref{ch:experiments} to simplify
experimenting with metrics on a variety of architectures, but those were mostly
not state-of-the-art models. To validate all findings, larger and more modern
architectures should be tested, alongside a larger variety of hpyerparameters,
for instance a larger range of learning rates and batch sizes, since they can
have a strong influence on the gradient distribution and thus the learning
process.

The experiments on learning rate optimisation
(\cref{sec:detecting_learning_rate_problems}) showed that a static target ratio
between updates and weights may not be the best approach.  Therefore, the
experiments should be extended with an exploration of whether a moving target
ratio leads to faster convergence, before the idea is entirely abandoned.

For the convergence experiment in \cref{sec:detecting_layer_saturation}, an
extensive search for the best threshold for freezing layers should be conducted
in order to optimise the tradeoff between compute time invested for computing
the metric and compute time saved by terminating training early.

\section{Tracking Second-Order Information}%
\label{sec:tracking_second_order_information}

While the success of purely gradient-based methods is remarkable in the
light of the proliferation of local minima and saddle points in highly
non-convex objectives in many dimensions (for a review on the saddle-point problem, see
\citet{dauphin2014identifying}), second-order information about the curvature of
the loss function around the current point in parameters space would provide
more valuable guides for selecting the direction of the next update step in
parameter space.

The central problem in obtaining second-order information about the loss
function is the dimensionality of the parameter space. The Hessian $H$ of a
scalar function $f(\boldsymbol{\theta})$ of a parameter vector $\boldsymbol{\theta} \in
\mathcal{R}^d$ is of size $d\times d$. Even storing this matrix is infeasible
for networks of millions of parameters, let alone computing it.
But the Hessian is what e.g. the Newton algorithm requires in higher dimensions.

There exist some second-order algorithms for non-convex optimisation.  One such
algorithm is Limited Memory BFGS \citep{liu1989limited}, an adaptation of the
BFGS algorithm---which estimates the inverse Hessian with respect to all model
parameters---that remembers only some history of update steps and gradients.
Another method is Conjugate Gradient Descent (originally described by
\citet{fletcher1964function}), which does not require the Hessian explicitly,
but only needs to compute Hessian-vector products, which is much easier.

Still, these methods have so far not demonstrated better performance in practice than
variants of first-order gradient descent. It is still an active area of research
how the second derivative can aid in speeding up convergence of the optimisation
or avoid some of the guesswork involved in finding good step sizes.

The library developed in this work has been used to track eigenvalues of the
Hessian estimated via deflated stochastic power iteration. For a diagonalisable
matrix $A$ and an initial estimate $\mathbf{q}_0$ with unit norm, the power method
(also known as Von-Mises iteration) computes the iterate
\begin{align}
    \mathbf{q}_k &= \frac{A\mathbf{q}_{k-1}}{||A\mathbf{q}_{k-1}||}_2
\end{align}
This series converges to the dominant eigenvector $\mathbf{v}_1$ of $A$ or not at all.
The corresponding eigenvalue $\lambda_1$ can be computed as
\begin{align}
    \lambda_1 = \frac{\left(A\mathbf{v}_1\right)^T\mathbf{v}_1}{\mathbf{v}^T\mathbf{v}}
\end{align}
per the definition of the eigenvalue. Deflation can then be used to obtain a
matrix $B$ whose dominant eigenvalue is the second largest eigenvalue of $A$.
\begin{align}
    B &= A - \lambda_1 \mathbf{v}_1\mathbf{v}_1^T
\end{align}
This allows computing the top-$k$ eigenpairs for any $k \le d$. The implementation by
\citet{golmant2018} uses PyTorch's \texttt{autograd} functionality to compute
the Hessian-Vector product $H\mathbf{q_k}$ with $\mathbf{q}_0 \sim U(0,1)$. The
estimate is stochastic since it used a fixed number of batches from the dataset
instead of all samples.

The functionality is realised in the \code{HessianEigenSubscriber} in the
\code{ikkuna. export.subscriber} subpackge. Eigenvalues and eigenvectors could
be used for directing the gradient descent process (\citet{alain2018negative}
tentatively find that largest decreases of the loss can often be made when
stepping along the most negative eigendirection, i.e. the most negative
curvature), but this would be an active intervention into the training, which is
not the goal of the library. More practically however, the Hessian eigenvalues
carry information about the sharpness of a local minimum and could be used for
diagnosing stability of the current mimiser, which is relevant both for
generalisation ability and resilience against adversarial attacks (inputs
crafted to fool the network). There has been some recent work arguing for
smaller batch sizes---one of the parameters whose choice we want to simplify
for the user---as they tend to generalise better (see e.g.
\citep{keskar2016large}). The claim is disputed, but \citet{yao2018hessian} find
that larger batch sizes during training strongly correlate with larger dominant
eigenvalues of the Hessian. It is unclear if an absolute value can be determined
for a given model and loss function at which the recommendation to reduce the
batch size can be made, but this is an interesting area for future research.
Analogously to \cref{sub:effects_update_to_weight_ratio_on_training_loss} one
could employ \texttt{ikkuna} to track the largest Hessian eigenvalue and a an
adversarial susceptibility score. The score could be computed as the average
decrease in accuracy over a set of image pairs where one of each pair is
adversarially perturbed. We can then discover correlations between metrics such
as the dominant Hessian eigenvalue and the vulnerability of a model to
adversarial attacks.


\section{Tracking Architectural Issues}%
\label{sec:tracking_architecture}

On of the primary difficulties in developing a neural network model to solve a
given problem is coming up with an architecture to begin with. Significant
advancements of the state of the art have been achieved by novel architectures,
such as the VGG, ResNet, ResNeXt, DenseNet or SqueezeNet, which highlights the
importance of finding a good number, size and type of layers.
\cref{sec:detecting_layer_saturation} discussed an approach for systematically freezing
layers, and similar lines of inquiry lead to the question of whether we can
track in how far the network is too constrained by its layer sizes, or too
powerful and thus harder to train. \citet{shenk2018} explored how ideas from
\citep{raghu2017svcca} can be used for identifying such problems. Given we can
detect when a layer is too large or too small, we could either abort and restart
training with a smaller architecture, thus saving time on iteration, or even
resize layers live. It is not obvious how to do this properly, since only the
insignificant components of a layer should be removed. We would need to identify
a projection of weight matrices onto a lower-dimensional subspace that does not
cancel all progress made throughout training.


\section{Improvements To The Software}%
\label{sec:improvements_to_the_software}

The \texttt{ikkuna} library simplifies metric tracking for neural networks, but
more work is necessary to make it production-ready, including bug
fixes\footnote{An updated list of open issues can  be found at
\url{https://github.com/Peltarion/ai_ikkuna/issues}}.

The following list lists useful features which should be implemented in future
versions.

\begin{itemize}
    \item Subscribers should be asynchronous and interleave their---possibly
        expensive---computations with the actual training. This should reduce
        the cost of computing metrics to almost nothing, since the main process
        is generally waiting while the GPU is working, besides loading and
        decoding training data---which again involves waiting on the hard disk.
    \item The API for now is purely callback-based---a result of the PyTorch
        API. However, for some use cases---notably computing validation
        accuracy and the SVCCA measure, which require the Subscriber to initiate
        forward passes---it is desirable to obtain activations or gradients as
        a return value of the forward pass, and not having to wait for the
        messages in a callback. An additional abstraction between the model and
        the subscriber could buffer the messages and return them in response to
        method call.
    \item A related problem is that some subscribers need direct access to the
        model to push data through it. This is undesirable as it couples
        Subscriber and Publisher. A service accepting data and returning
        intermediate activations and gradients would we a good complement to the
        callback-based API.
    \item The relationship between the \texttt{Subscriber} and
        \texttt{Subscription} classes is convoluted at present and can probably
        be simplified.
\end{itemize}
