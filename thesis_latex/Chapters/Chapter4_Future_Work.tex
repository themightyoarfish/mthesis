\chapter{Future Work}%
\label{cha:future_work}

This chapter will examine some avenues of inquiry which for several reasons are
beyond the scope of this work.

\section{Tracking Second-Order Information}%
\label{sec:tracking_second_order_information}

While the success of purely gradient-based methods is remarkable in the
light of the proliferation of local minima and saddle points in highly
non-convex objectives in many dimensions (for a review on the saddle-point problem, see
\citet{dauphin2014identifying}), second-order information about the curvature of
the loss function around the current point in parameters space would provide
more valuable guides for selecting the direction of the next update step in
parameter space.

The central problem in obtaining second-order information about the loss
function is the dimensionality of the parameter space. The Hessian $H$ of a
scalar function $f(\boldsymbol{\theta})$ of a parameter vector $\boldsymbol{\theta} \in
\mathcal{R}^d$ is of size $d\times d$. Even storing this matrix is infeasible
for networks of millions of parameters, let alone computing it.
But the Hessian is what e.g. the Newton algorithm requires in higher dimensions.

There exist some second-order algorithms for non-convex optimisation.  One such
algorithm is Limited Memory BFGS \citep{liu1989limited}, an adaptation of the
BFGS algorithm -- which estimates the inverse Hessian with respect to all model
parameters -- that remembers only some history of update steps and gradients.
Another method is Conjugate Gradient Descent (originally described by
\citet{fletcher1964function}), which does not require the Hessian explicitly,
but only needs to compute Hessian-vector products, which is much easier.

Still, these methods have so far not demonstrated better performance in practice than
variants of first-order gradient descent. It is still an active area of research
how the second derivative can aid in speeding up convergence of the optimisation
or avoid some of the guesswork involved in finding good step sizes.

The library developed in this work has been used to track eigenvalues of the
Hessian estimated via deflated stochastic power iteration. For a diagonalisable
matrix $A$ and an initial estimate $\mathbf{q}_0$ with unit norm, the power method
(also known as Von-Mises iteration) computes the iterate
\begin{align}
    \mathbf{q}_k &= \frac{A\mathbf{q}_{k-1}}{||A\mathbf{q}_{k-1}||}_2
\end{align}
This series converges to the dominant eigenvector $\mathbf{v}_1$ of $A$ or not at all.
The corresponding eigenvalue $\lambda_1$ can be computed as
\begin{align}
    \lambda_1 = \frac{\left(A\mathbf{v}_1\right)^T\mathbf{v}_1}{\mathbf{v}^T\mathbf{v}}
\end{align}
per the definition of the eigenvalue. Deflation can then be used to obtain a
matrix $B$ whose dominant eigenvalue is the second largest eigenvalue of $A$.
\begin{align}
    B &= A - \lambda_1 \mathbf{v}_1\mathbf{v}_1^T
\end{align}
This allows computing the top-$k$ eigenpairs for any $k \le d$. The implementation by
\citet{golmant2018} uses PyTorch's \texttt{autograd} functionality to compute
the Hessian-Vector product $H\mathbf{q_k}$ with $\mathbf{q}_0 \sim U(0,1)$. The
estimate is stochastic since it used a fixed number of batches from the dataset
instead of all samples.

The functionality is realised in the \code{HessianEigenSubscriber} in the
\code{ikkuna. export.subscriber} subpackge. Eigenvalues and eigenvectors could
be used for directing the gradient descent process (\citet{alain2018negative}
tentatively find that largest decreases of the loss can often be made when
stepping along the most negative eigendirection, i.e. the most negative
curvature), but this would be an active intervention into the training, which is
not the goal of the library. More practically however, the hessian eigenvalues
carry information about the sharpness of a local minimum and could be used for
diagnosing stability of the current mimiser, which is relevant both for
generalisation ability and resilience against adversarial attacks (inputs
crafted to fool the network). There has been some recent work arguing for
smaller batch sizes -- one of the parameters whose choice we want to simplify
for the user -- as they tend to generalise better (see e.g.
\citep{keskar2016large}). The claim is disputed, but \citet{yao2018hessian} find
that larger batch sizes during training strongly correlate with larger dominant
eigenvalues of the Hessian. It is unclear if an absolute value can be determined
for a given model and loss function at which the recommendation to reduce the
batch size can be made, but this is an interesting area for future research.
Analogously to \cref{sub:effects_update_to_weight_ratio_on_training_loss} one
could employ \texttt{ikkuna} to track the largest hessian eigenvalue and a an
adversarial susceptibility score. The score could be computed as the average
decrease in accuracy over a set of image pairs where one of each pair is
adversarially perturbed. We can then discover correlations between metrics such
as the dominant hessian eigenvalue and the vulnerability of a model to
adversarial attacks.


\section{Tracking Architectural Issues}%
\label{sec:tracking_architecture}

On of the primary difficulties in developing a neural network model to solve a
given problem is coming up with an architecture to begin with. Significant
advancements of the state of the art have been achieved by novel architectures,
such as the VGG, ResNet, ResNeXt, DenseNet or SqueezeNet, which highlights the
importance of finding a good number, size and type of layers.
\cref{sec:layer_saturation} discussed an approach for systematically freezing
layers, and similar lines of inquiry lead to the question of whether we can
track in how far the network is too constrained by its layer sizes, or too
powerful and thus harder to train. \citet{shenk2018} explored how ideas from
\citep{raghu2017svcca} can be used for identifying such problems. Given we can
detect when a layer is too large or too small, we could either abort and restart
training with a smaller architecture, thus saving time on iteration, or even
resize layers live. It is not obvious how to do this properly, since only the
insignificant components of a layer should be removed. We would need to identify
a projection of weight matrices on a lower-dimennsional subspace that does not
cancel all progress made throughout training.
